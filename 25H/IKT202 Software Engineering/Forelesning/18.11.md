Validation vs Verification
Validation - did we make the right software
- Validation against business needs, high level requirements, business issue.
Verification - did we make the software right
- Does the implementation, architecture, design meet the requirements.
- The software conforms to the specification.
- Tech issue.

Black box testing
	Testing without the reference to the internal structure of the component or system
The tester observes or measures the output, based on the input.
The definition of the components API will define that stimuli will cause what reaction

Used in
- Integration testing
- System testing
- Acceptance testing
Advantages
- Can be written before the code is complete (or started)
- Can help developers in development
- Can be reused across implementations.

White box testing
- Testing based on the analysis of the internal structure of the component or system.
- Selects stimuli and expected response based on the intimate knowledge of the inner working.
Used in 
- unit testing
- integration testing
- system testing
advantages
- can start execution earlier, e.g. unit testing
- more thorough with the possibility of covering more code coverage.
Disadvantages
- complex
- highly dependent on implementation -> change when inner code change.

Static analysis and dynamic
static analysis: testing without executing the program
- code walk-through and inspection
- various analysis
Dynamic testing: testing by execution the program with real input
	static information can be used to make dynamic testing more efficient, white-box graybox testing
-implementing the logic in a formal language
	spin/promela

Test plans
- master test plan
	- Overall test approach
	- Documents the what, the why the when, and the who, for all levels of dynamic testing.
- Detailed test plan
	- Plan for each component or sub-system for each level of testing, this can be based on organisation work package etc.
	- What will be tested, who will perform, when and how.
- Test preparation
	- gather test data.
	- etc.

Example architecture
- Focus areas
	- UI (different owners)
	- Data processing
	- Analytics
	- Raw data acquisition

Reporting in fl. 
Data interface ( ask for data upstream) 
streaming interface (continuous, downstream)

Technical readiness level (TRL)e
0 - discovery of mathematical principle or algorithm, 
1 - application envisioned and described, (conceptual refinement
2 - proof of concept, critical functionality empirically confirmed and implemented.
3 -  component tested in simulation, producibility and cost estimated, software component tested in an integration laboratory.
4 - prototype tested in relevant environment and tested in a limited field environment
5 - system tested in a relevant environment
6 - system tested in an operation environment
7 - production system tested in an operational environment.

TRL additions 1
TRL level    scale of testing    fidelity    environment    trl goal
    0                            paper
    1                            paper
	2          lab               piece
	3          lab               piece
	4          engineering       piece 
	5          engineering       similar
	6          full              full
	7          full              full
lab < 1/10 full scale
engineering scale 1/10 < system < full scale
full scale, matches final system/application

Master testplan enter/exit criteria
- component testing
	- entry criteria
		- all source code for the new component has passed unit testing
	- exit
		- testing at TRL 3
		- no defects of severity level 1 or 2 found
		- documentation updated and reflect state of components
- integration testing.
	- entry criteria
		- passed TRL 3
	- exit criteria
		- testing at TRL 4
- system testing
	- entry criteria:
	- exit criteria
		- testing at TRL 5
- Acceptance testing:
	- entry criteria:
	- exit criteria:
		- testing at TRL 6

Unit (component) testing
- first systematic test
- In OO world this may be module, unit, class test
- Software units are tested isolated from all other units of the system
	- important to prevent external influence
- a unit may be composed of several other units.
- usually need framework or harness.
- often closely related to debugging, but debugging is not testing.
	- debugging is more root cause analysis

Positive and negative tests.
Test objectives
- ensure functional correctness
- ensure coverage.
- hit all paths of the code.
- all states of ifs, all switches, etc.
bugs are usually:
	logical programming faults
	edges/special cases