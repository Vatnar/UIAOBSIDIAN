Validation vs Verification
Validation - did we make the right software
- Validation against business needs, high level requirements, business issue.
Verification - did we make the software right
- Does the implementation, architecture, design meet the requirements.
- The software conforms to the specification.
- Tech issue.

Black box testing
	Testing without the reference to the internal structure of the component or system
The tester observes or measures the output, based on the input.
The definition of the components API will define that stimuli will cause what reaction

Used in
- Integration testing
- System testing
- Acceptance testing
Advantages
- Can be written before the code is complete (or started)
- Can help developers in development
- Can be reused across implementations.

White box testing
- Testing based on the analysis of the internal structure of the component or system.
- Selects stimuli and expected response based on the intimate knowledge of the inner working.
Used in 
- unit testing
- integration testing
- system testing
advantages
- can start execution earlier, e.g. unit testing
- more thorough with the possibility of covering more code coverage.
Disadvantages
- complex
- highly dependent on implementation -> change when inner code change.

Statifca analysis and dynamic
static analysis: testing without executing hte prgoram
- code walkthrough and inspection
- various analyasis
Dynamic testing: testing by execution the program with real input
	static information can be used to make dynamic testing more efficient, whitebox graybox testing
-implementing the logic in a formal language
	spin promela

Test plans
- master testplan
	- Overall test approach
	- Documents the what, the why the when, and the who, for all levels of dynamic testing.
- Detailed testplan
	- PLan for each component or sub-system for each level of teting, this can be based on organization work package etc.
	- What will be tested, who will perform, when and how.
- Test preparation
	- gather test data.
	- etc.

Example architecture
- Focus areas
	- UI (different owners)
	- Data processing
	- Analytics
	- Raw data aquisition

Reporting in fl. 
Data interface ( ask for data upstream) 
streaming interface (continous, downstream)

Techinical readiness level (TRL)e
0 - discovery of mathematical principle or algorithm, 
1 - application envisioned and described, (conceptual refinement
2 - proof of concept, critical functionality empirically confirmed and implemented.
3 -  compoenent tested in simulation, producibility and cost estimated, software compoenent tested in an integration laboratory.
4 - protoyp tested in releveant envornment and tested ina litmited fielt environment
5 - system tested in a relevant enviornment
6 - system tested in an operation environment
7 - production system tested in an operational environment.

TRL additions 1
TRL level    scale of testing    fidelity    environment    trl goal
    0                            
    1                            
	2          lab
	3          lab
	4          engineering
	5          engineering
	6          full
	7          full
lab < 1/10 full scale
engineering scale 1/10 < system < full scale
full scale, matches final system/application